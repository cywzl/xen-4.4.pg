Cause to be defined all those extra Xen symbol offsets that kdump needs
to navigate its way around a memory image.

diff -r e0a00bff32e0 xen/Makefile
--- a/xen/Makefile
+++ b/xen/Makefile
@@ -188,6 +188,7 @@ _cscope:
 .PHONY: _MAP
 _MAP:
 	$(NM) -n $(TARGET)-syms | grep -v '\(compiled\)\|\(\.o$$\)\|\( [aUw] \)\|\(\.\.ng$$\)\|\(LASH[RL]DI\)' > System.map
+	cat include/asm/asm-offsets.h | awk '/^#define __ASM_OFFSETS_H__/ { next } ; /^#define / { printf "%016x - +%s\n", $$3, $$2 }' >> System.map
 
 .PHONY: FORCE
 FORCE:
diff -r e0a00bff32e0 xen/arch/x86/x86_32/asm-offsets.c
--- a/xen/arch/x86/x86_32/asm-offsets.c
+++ b/xen/arch/x86/x86_32/asm-offsets.c
@@ -51,6 +51,31 @@ void __dummy__(void)
     DEFINE(UREGS_user_sizeof, sizeof(struct cpu_user_regs));
     BLANK();
 
+    OFFSET(DOMAIN_id, struct domain, domain_id);
+    OFFSET(DOMAIN_shared_info, struct domain, shared_info);
+    OFFSET(DOMAIN_next, struct domain, next_in_list);
+    OFFSET(DOMAIN_vcpus, struct domain, vcpu);
+    OFFSET(DOMAIN_is_hvm, struct domain, is_hvm);
+    OFFSET(DOMAIN_is_privileged, struct domain, is_privileged);
+    OFFSET(DOMAIN_is_32bit_pv, struct domain, arch.is_32bit_pv);
+    OFFSET(DOMAIN_tot_pages, struct domain, tot_pages);
+    OFFSET(DOMAIN_max_pages, struct domain, max_pages);
+    OFFSET(DOMAIN_shr_pages, struct domain, shr_pages);
+    OFFSET(DOMAIN_has_32bit_shinfo, struct domain, arch.has_32bit_shinfo);
+    OFFSET(DOMAIN_handle, struct domain, handle);
+    OFFSET(DOMAIN_paging_mode, struct domain, arch.paging.mode);
+    DEFINE(DOMAIN_sizeof, sizeof(struct domain));
+    BLANK();
+
+    OFFSET(SHARED_max_pfn, struct shared_info, arch.max_pfn);
+    OFFSET(SHARED_pfn_to_mfn_list_list, struct shared_info, arch.pfn_to_mfn_frame_list_list);
+    BLANK();
+
+    DEFINE(XEN_virt_start, __XEN_VIRT_START);
+    DEFINE(XEN_page_offset, __PAGE_OFFSET);
+    BLANK();
+
+    OFFSET(VCPU_vcpu_id, struct vcpu, vcpu_id);
     OFFSET(VCPU_processor, struct vcpu, processor);
     OFFSET(VCPU_vcpu_info, struct vcpu, vcpu_info);
     OFFSET(VCPU_trap_bounce, struct vcpu, arch.pv_vcpu.trap_bounce);
@@ -64,6 +89,12 @@ void __dummy__(void)
     OFFSET(VCPU_kernel_ss, struct vcpu, arch.pv_vcpu.kernel_ss);
     OFFSET(VCPU_kernel_sp, struct vcpu, arch.pv_vcpu.kernel_sp);
     OFFSET(VCPU_guest_context_flags, struct vcpu, arch.vgc_flags);
+    OFFSET(VCPU_user_regs, struct vcpu, arch.user_regs);
+    OFFSET(VCPU_guest_table, struct vcpu, arch.guest_table);
+    OFFSET(VCPU_cr3, struct vcpu, arch.cr3);
+    OFFSET(VCPU_domain, struct vcpu, domain);
+    DEFINE(VCPU_sizeof, sizeof(struct vcpu));
+    OFFSET(VCPU_pause_flags, struct vcpu, pause_flags);
     OFFSET(VCPU_nmi_pending, struct vcpu, nmi_pending);
     OFFSET(VCPU_mce_pending, struct vcpu, mce_pending);
     OFFSET(VCPU_nmi_old_mask, struct vcpu, nmi_state.old_mask);
@@ -111,6 +142,7 @@ void __dummy__(void)
     OFFSET(CPUINFO_guest_cpu_user_regs, struct cpu_info, guest_cpu_user_regs);
     OFFSET(CPUINFO_processor_id, struct cpu_info, processor_id);
     OFFSET(CPUINFO_current_vcpu, struct cpu_info, current_vcpu);
+    OFFSET(CPUINFO_per_cpu_offset, struct cpu_info, per_cpu_offset);
     DEFINE(CPUINFO_sizeof, sizeof(struct cpu_info));
     BLANK();
 
diff -r e0a00bff32e0 xen/arch/x86/x86_64/asm-offsets.c
--- a/xen/arch/x86/x86_64/asm-offsets.c
+++ b/xen/arch/x86/x86_64/asm-offsets.c
@@ -60,8 +60,38 @@ void __dummy__(void)
     DEFINE(UREGS_user_sizeof, sizeof(struct cpu_user_regs));
     BLANK();
 
+    OFFSET(DOMAIN_id, struct domain, domain_id);
+    OFFSET(DOMAIN_shared_info, struct domain, shared_info);
+    OFFSET(DOMAIN_next, struct domain, next_in_list);
+    OFFSET(DOMAIN_max_vcpus, struct domain, max_vcpus);
+    OFFSET(DOMAIN_vcpus, struct domain, vcpu);
+    OFFSET(DOMAIN_is_hvm, struct domain, is_hvm);
+    OFFSET(DOMAIN_is_privileged, struct domain, is_privileged);
+    OFFSET(DOMAIN_tot_pages, struct domain, tot_pages);
+    OFFSET(DOMAIN_max_pages, struct domain, max_pages);
+    OFFSET(DOMAIN_shr_pages, struct domain, shr_pages);
+    OFFSET(DOMAIN_has_32bit_shinfo, struct domain, arch.has_32bit_shinfo);
+    OFFSET(DOMAIN_handle, struct domain, handle);
+    OFFSET(DOMAIN_paging_mode, struct domain, arch.paging.mode);
+    DEFINE(DOMAIN_sizeof, sizeof(struct domain));
+    BLANK();
+
+    OFFSET(SHARED_max_pfn, struct shared_info, arch.max_pfn);
+    OFFSET(SHARED_pfn_to_mfn_list_list, struct shared_info, arch.pfn_to_mfn_frame_list_list);
+    BLANK();
+
+    DEFINE(XEN_virt_start, __XEN_VIRT_START);
+    DEFINE(XEN_page_offset, __PAGE_OFFSET);
+#ifndef NDEBUG
+    DEFINE(XEN_DEBUG, 1);
+#else
+    DEFINE(XEN_DEBUG, 0);
+#endif
+    BLANK();
+
     OFFSET(irq_caps_offset, struct domain, irq_caps);
     OFFSET(next_in_list_offset, struct domain, next_in_list);
+    OFFSET(VCPU_vcpu_id, struct vcpu, vcpu_id);
     OFFSET(VCPU_processor, struct vcpu, processor);
     OFFSET(VCPU_domain, struct vcpu, domain);
     OFFSET(VCPU_vcpu_info, struct vcpu, vcpu_info);
@@ -92,7 +122,10 @@ void __dummy__(void)
     OFFSET(VCPU_kernel_sp, struct vcpu, arch.pv_vcpu.kernel_sp);
     OFFSET(VCPU_kernel_ss, struct vcpu, arch.pv_vcpu.kernel_ss);
     OFFSET(VCPU_guest_context_flags, struct vcpu, arch.vgc_flags);
+    OFFSET(VCPU_user_regs, struct vcpu, arch.user_regs);
+    OFFSET(VCPU_cr3, struct vcpu, arch.cr3);
     OFFSET(VCPU_nmi_pending, struct vcpu, nmi_pending);
+    OFFSET(VCPU_pause_flags, struct vcpu, pause_flags);
     OFFSET(VCPU_mce_pending, struct vcpu, mce_pending);
     OFFSET(VCPU_nmi_old_mask, struct vcpu, nmi_state.old_mask);
     OFFSET(VCPU_mce_old_mask, struct vcpu, mce_state.old_mask);
@@ -101,6 +134,7 @@ void __dummy__(void)
     DEFINE(VCPU_TRAP_MCE, VCPU_TRAP_MCE);
     DEFINE(_VGCF_failsafe_disables_events, _VGCF_failsafe_disables_events);
     DEFINE(_VGCF_syscall_disables_events,  _VGCF_syscall_disables_events);
+    DEFINE(VCPU_sizeof, sizeof(struct vcpu));
     BLANK();
 
     OFFSET(VCPU_svm_vmcb_pa, struct vcpu, arch.hvm_svm.vmcb_pa);
@@ -140,6 +174,7 @@ void __dummy__(void)
     OFFSET(CPUINFO_guest_cpu_user_regs, struct cpu_info, guest_cpu_user_regs);
     OFFSET(CPUINFO_processor_id, struct cpu_info, processor_id);
     OFFSET(CPUINFO_current_vcpu, struct cpu_info, current_vcpu);
+    OFFSET(CPUINFO_per_cpu_offset, struct cpu_info, per_cpu_offset);
     DEFINE(CPUINFO_sizeof, sizeof(struct cpu_info));
     BLANK();
 
