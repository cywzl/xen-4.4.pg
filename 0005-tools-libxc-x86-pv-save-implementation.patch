From f51e38045d82ec88ea514d62a5613604240a6c14 Mon Sep 17 00:00:00 2001
From: Andrew Cooper <andrew.cooper3@citrix.com>
Date: Wed, 19 Mar 2014 12:39:25 +0000
Subject: [PATCH 5/6] tools/libxc: x86 pv save implementation

Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
Signed-off-by: Frediano Ziglio <frediano.ziglio@citrix.com>
---
 tools/libxc/saverestore/common.c      |   36 ++
 tools/libxc/saverestore/common.h      |   52 ++
 tools/libxc/saverestore/save.c        |   33 +-
 tools/libxc/saverestore/save_x86_pv.c | 1111 +++++++++++++++++++++++++++++++++
 4 files changed, 1231 insertions(+), 1 deletion(-)
 create mode 100644 tools/libxc/saverestore/save_x86_pv.c

diff -r e238f83e7197 tools/libxc/saverestore/common.c
--- a/tools/libxc/saverestore/common.c
+++ b/tools/libxc/saverestore/common.c
@@ -1,3 +1,5 @@
+#include <assert.h>
+
 #include "common.h"
 
 static const char *dhdr_types[] =
@@ -49,6 +51,40 @@ const char *rec_type_to_str(uint32_t typ
     return "Reserved";
 }
 
+int write_split_record(struct context *ctx, struct record *rec,
+                       void *buf, size_t sz)
+{
+    static const char zeroes[7] = { 0 };
+    xc_interface *xch = ctx->xch;
+    uint32_t combined_length = rec->length + sz;
+    size_t record_length = (combined_length + 7) & ~7UL;
+
+    if ( record_length > REC_LENGTH_MAX )
+    {
+        ERROR("Record (0x%08"PRIx32", %s) length 0x%"PRIx32
+              " exceeds max (0x%"PRIx32")", rec->type,
+              rec_type_to_str(rec->type), rec->length, REC_LENGTH_MAX);
+        return -1;
+    }
+
+    if ( rec->length )
+        assert(rec->data);
+    if ( sz )
+        assert(buf);
+
+    if ( write_exact(ctx->fd, &rec->type, sizeof rec->type) ||
+         write_exact(ctx->fd, &combined_length, sizeof rec->length) ||
+         (rec->length && write_exact(ctx->fd, rec->data, rec->length)) ||
+         (sz && write_exact(ctx->fd, buf, sz)) ||
+         write_exact(ctx->fd, zeroes, record_length - combined_length) )
+    {
+        PERROR("Unable to write record to stream");
+        return -1;
+    }
+
+    return 0;
+}
+
 /*
  * Local variables:
  * mode: C
diff -r e238f83e7197 tools/libxc/saverestore/common.h
--- a/tools/libxc/saverestore/common.h
+++ b/tools/libxc/saverestore/common.h
@@ -10,6 +10,8 @@
 #include "../xc_dom.h"
 #include "../xc_bitops.h"
 
+#undef GET_FIELD
+#undef SET_FIELD
 #undef mfn_to_pfn
 #undef pfn_to_mfn
 
@@ -97,6 +99,56 @@ struct context
     };
 };
 
+/* Saves an x86 PV domain. */
+int save_x86_pv(struct context *ctx);
+
+struct record
+{
+    uint32_t type;
+    uint32_t length;
+    void *data;
+};
+
+/* Gets a field from an *_any union */
+#define GET_FIELD(_c, _p, _f)                   \
+    ({ (_c)->x86_pv.width == 8 ?                \
+            (_p)->x64._f:                       \
+            (_p)->x32._f;                       \
+    })                                          \
+
+/* Gets a field from an *_any union */
+#define SET_FIELD(_c, _p, _f, _v)               \
+    ({ if ( (_c)->x86_pv.width == 8 )           \
+            (_p)->x64._f = (_v);                \
+        else                                    \
+            (_p)->x32._f = (_v);                \
+    })
+
+/*
+ * Writes a split record to the stream, applying correct padding where
+ * appropriate.  It is common when sending records containing blobs from Xen
+ * that the header and blob data are separate.  This function accepts a second
+ * buffer and length, and will merge it with the main record when sending.
+ *
+ * Records with a non-zero length must provide a valid data field; records
+ * with a 0 length shall have their data field ignored.
+ *
+ * Returns 0 on success and non0 on failure.
+ */
+int write_split_record(struct context *ctx, struct record *rec, void *buf, size_t sz);
+
+/*
+ * Writes a record to the stream, applying correct padding where appropriate.
+ * Records with a non-zero length must provide a valid data field; records
+ * with a 0 length shall have their data field ignored.
+ *
+ * Returns 0 on success and non0 on failure.
+ */
+static inline int write_record(struct context *ctx, struct record *rec)
+{
+    return write_split_record(ctx, rec, NULL, 0);
+}
+
 #endif
 /*
  * Local variables:
diff -r e238f83e7197 tools/libxc/saverestore/save.c
--- a/tools/libxc/saverestore/save.c
+++ b/tools/libxc/saverestore/save.c
@@ -5,8 +5,39 @@ int xc_domain_save2(xc_interface *xch, i
                     struct save_callbacks* callbacks, int hvm,
                     unsigned long vm_generationid_addr)
 {
+    struct context ctx =
+        {
+            .xch = xch,
+            .fd = io_fd,
+        };
+
+    /* Older GCC cant initialise anonymous unions */
+    ctx.save.callbacks = callbacks;
+
     IPRINTF("In experimental %s", __func__);
-    return -1;
+
+    if ( xc_domain_getinfo(xch, dom, 1, &ctx.dominfo) != 1 )
+    {
+        PERROR("Failed to get domain info");
+        return -1;
+    }
+
+    if ( ctx.dominfo.domid != dom )
+    {
+        ERROR("Domain %d does not exist", dom);
+        return -1;
+    }
+
+    ctx.domid = dom;
+    IPRINTF("Saving domain %d", dom);
+
+    if ( ctx.dominfo.hvm )
+    {
+        ERROR("HVM Save not supported yet");
+        return -1;
+    }
+    else
+        return save_x86_pv(&ctx);
 }
 
 /*
diff -r e238f83e7197 tools/libxc/saverestore/save_x86_pv.c
--- /dev/null
+++ b/tools/libxc/saverestore/save_x86_pv.c
@@ -0,0 +1,1111 @@
+#include <assert.h>
+#include <arpa/inet.h>
+
+#include "common_x86_pv.h"
+
+static int write_headers(struct context *ctx)
+{
+    xc_interface *xch = ctx->xch;
+    int32_t xen_version = xc_version(xch, XENVER_version, NULL);
+    struct ihdr ihdr =
+        {
+            .marker  = IHDR_MARKER,
+            .id      = htonl(IHDR_ID),
+            .version = htonl(IHDR_VERSION),
+            .options = htons(IHDR_OPT_LITTLE_ENDIAN),
+        };
+    struct dhdr dhdr =
+        {
+            .type       = DHDR_TYPE_x86_pv,
+            .page_shift = 12,
+            .xen_major  = (xen_version >> 16) & 0xffff,
+            .xen_minor  = (xen_version)       & 0xffff,
+        };
+
+    if ( xen_version < 0 )
+    {
+        PERROR("Unable to obtain Xen Version");
+        return -1;
+    }
+
+    if ( write_exact(ctx->fd, &ihdr, sizeof ihdr) )
+    {
+        PERROR("Unable to write Image Header to stream");
+        return -1;
+    }
+
+    if ( write_exact(ctx->fd, &dhdr, sizeof dhdr) )
+    {
+        PERROR("Unable to write Domain Header to stream");
+        return -1;
+    }
+
+    return 0;
+}
+
+static int map_shinfo(struct context *ctx)
+{
+    xc_interface *xch = ctx->xch;
+
+    ctx->x86_pv.shinfo = xc_map_foreign_range(
+        xch, ctx->domid, PAGE_SIZE, PROT_READ, ctx->dominfo.shared_info_frame);
+    if ( !ctx->x86_pv.shinfo )
+    {
+        PERROR("Failed to map shared info frame at pfn %#lx",
+               ctx->dominfo.shared_info_frame);
+        return -1;
+    }
+
+    return 0;
+}
+
+static void copy_pfns_from_guest(struct context *ctx, xen_pfn_t *dst,
+                                 void *src, size_t count)
+{
+    size_t x;
+
+    if ( ctx->x86_pv.width == sizeof(unsigned long) )
+        memcpy(dst, src, count * sizeof *dst);
+    else
+    {
+        for ( x = 0; x < count; ++x )
+        {
+#ifdef __x86_64__
+            /* 64bit toolstack, 32bit guest.  Expand any INVALID_MFN. */
+            uint32_t s = ((uint32_t *)src)[x];
+
+            dst[x] = s == ~0U ? INVALID_MFN : s;
+#else
+            /* 32bit toolstack, 64bit guest.  Truncate their pointers */
+            dst[x] = ((uint64_t *)src)[x];
+#endif
+        }
+    }
+}
+
+static int map_p2m(struct context *ctx)
+{
+    /* Terminology:
+     *
+     * fll   - frame list list, top level p2m, list of fl mfns
+     * fl    - frame list, mid level p2m, list of leaf mfns
+     * local - own allocated buffers, adjusted for bitness
+     * guest - mappings into the domain
+     */
+    xc_interface *xch = ctx->xch;
+    int rc = -1;
+    unsigned tries = 100, x, fpp, fll_entries, fl_entries;
+    xen_pfn_t fll_mfn;
+
+    xen_pfn_t *local_fll = NULL;
+    void *guest_fll = NULL;
+    size_t local_fll_size;
+
+    xen_pfn_t *local_fl = NULL;
+    void *guest_fl = NULL;
+    size_t local_fl_size;
+
+    fpp = ctx->x86_pv.fpp = PAGE_SIZE / ctx->x86_pv.width;
+    fll_entries = (ctx->x86_pv.max_pfn / (fpp * fpp)) + 1;
+    fl_entries  = (ctx->x86_pv.max_pfn / fpp) + 1;
+
+    fll_mfn = GET_FIELD(ctx, ctx->x86_pv.shinfo, arch.pfn_to_mfn_frame_list_list);
+    if ( !fll_mfn )
+        IPRINTF("Waiting for domain to set up its p2m frame list list");
+
+    while ( tries-- && !fll_mfn )
+    {
+        usleep(10000);
+        fll_mfn = GET_FIELD(ctx, ctx->x86_pv.shinfo,
+                            arch.pfn_to_mfn_frame_list_list);
+    }
+
+    if ( !fll_mfn )
+    {
+        ERROR("Timed out waiting for p2m frame list list to be updated");
+        goto err;
+    }
+
+    /* Map the guest top p2m */
+    guest_fll = xc_map_foreign_range(xch, ctx->domid, PAGE_SIZE,
+                                     PROT_READ, fll_mfn);
+    if ( !guest_fll )
+    {
+        PERROR("Failed to map p2m frame list list at %#lx", fll_mfn);
+        goto err;
+    }
+
+    local_fll_size = fll_entries * sizeof *local_fll;
+    local_fll = malloc(local_fll_size);
+    if ( !local_fll )
+    {
+        ERROR("Cannot allocate %zu bytes for local p2m frame list list",
+              local_fll_size);
+        goto err;
+    }
+
+    copy_pfns_from_guest(ctx, local_fll, guest_fll, fll_entries);
+
+    /* Map the guest mid p2m frames */
+    guest_fl = xc_map_foreign_pages(xch, ctx->domid, PROT_READ,
+                                    local_fll, fll_entries);
+    if ( !guest_fl )
+    {
+        PERROR("Failed to map p2m frame list");
+        goto err;
+    }
+
+    local_fl_size = fl_entries * sizeof *local_fl;
+    local_fl = malloc(local_fl_size);
+    if ( !local_fl )
+    {
+        ERROR("Cannot allocate %zu bytes for local p2m frame list",
+              local_fl_size);
+        goto err;
+    }
+
+    copy_pfns_from_guest(ctx, local_fl, guest_fl, fl_entries);
+
+    /* Map the p2m leaves themselves */
+    ctx->x86_pv.p2m = xc_map_foreign_pages(xch, ctx->domid, PROT_READ,
+                                           local_fl, fl_entries);
+    if ( !ctx->x86_pv.p2m )
+    {
+        PERROR("Failed to map p2m frames");
+        goto err;
+    }
+
+    ctx->x86_pv.p2m_frames = fl_entries;
+    ctx->x86_pv.p2m_pfns = malloc(local_fl_size);
+    if ( !ctx->x86_pv.p2m_pfns )
+    {
+        ERROR("Cannot allocate %zu bytes for p2m pfns list",
+              local_fl_size);
+        goto err;
+    }
+
+    /* Convert leaf frames from mfns to pfns */
+    for ( x = 0; x < fl_entries; ++x )
+        if ( !mfn_in_pseudophysmap(ctx, local_fl[x]) )
+        {
+            ERROR("Bad MFN in p2m_frame_list[%u]", x);
+            pseudophysmap_walk(ctx, local_fl[x]);
+            errno = ERANGE;
+            goto err;
+        }
+        else
+            ctx->x86_pv.p2m_pfns[x] = mfn_to_pfn(ctx, local_fl[x]);
+
+    rc = 0;
+err:
+
+    free(local_fl);
+    if ( guest_fl )
+        munmap(guest_fl, fll_entries * PAGE_SIZE);
+
+    free(local_fll);
+    if ( guest_fll )
+        munmap(guest_fll, PAGE_SIZE);
+
+    return rc;
+}
+
+static int write_one_vcpu_basic(struct context *ctx, uint32_t id)
+{
+    xc_interface *xch = ctx->xch;
+    xen_pfn_t mfn, pfn;
+    unsigned i;
+    int rc = -1;
+    vcpu_guest_context_any_t vcpu;
+    struct rec_x86_pv_vcpu vhdr = { .vcpu_id = id };
+    struct record rec =
+    {
+        .type = REC_TYPE_x86_pv_vcpu_basic,
+        .length = sizeof vhdr,
+        .data = &vhdr,
+    };
+
+    if ( xc_vcpu_getcontext(xch, ctx->domid, id, &vcpu) )
+    {
+        PERROR("Failed to get vcpu%u context", id);
+        goto err;
+    }
+
+    /* Vcpu 0 is special: Convert the suspend record to a PFN */
+    if ( id == 0 )
+    {
+        mfn = GET_FIELD(ctx, &vcpu, user_regs.edx);
+        if ( !mfn_in_pseudophysmap(ctx, mfn) )
+        {
+            ERROR("Bad MFN for suspend record");
+            pseudophysmap_walk(ctx, mfn);
+            errno = ERANGE;
+            goto err;
+        }
+        SET_FIELD(ctx, &vcpu, user_regs.edx, mfn_to_pfn(ctx, mfn));
+    }
+
+    /* Convert GDT frames to PFNs */
+    for ( i = 0; (i * 512) < GET_FIELD(ctx, &vcpu, gdt_ents); ++i )
+    {
+        mfn = GET_FIELD(ctx, &vcpu, gdt_frames[i]);
+        if ( !mfn_in_pseudophysmap(ctx, mfn) )
+        {
+            ERROR("Bad MFN for frame %u of vcpu%u's GDT", i, id);
+            pseudophysmap_walk(ctx, mfn);
+            errno = ERANGE;
+            goto err;
+        }
+        SET_FIELD(ctx, &vcpu, gdt_frames[i], mfn_to_pfn(ctx, mfn));
+    }
+
+    /* Convert CR3 to a PFN */
+    mfn = cr3_to_mfn(ctx, GET_FIELD(ctx, &vcpu, ctrlreg[3]));
+    if ( !mfn_in_pseudophysmap(ctx, mfn) )
+    {
+        ERROR("Bad MFN for vcpu%u's cr3", id);
+        pseudophysmap_walk(ctx, mfn);
+        errno = ERANGE;
+        goto err;
+    }
+    pfn = mfn_to_pfn(ctx, mfn);
+    SET_FIELD(ctx, &vcpu, ctrlreg[3], mfn_to_cr3(ctx, pfn));
+
+    /* 64bit guests: Convert CR1 (guest pagetables) to PFN */
+    if ( ctx->x86_pv.levels == 4 && vcpu.x64.ctrlreg[1] )
+    {
+        mfn = vcpu.x64.ctrlreg[1] >> PAGE_SHIFT;
+        if ( !mfn_in_pseudophysmap(ctx, mfn) )
+        {
+            ERROR("Bad MFN for vcpu%u's cr1", id);
+            pseudophysmap_walk(ctx, mfn);
+            errno = ERANGE;
+            goto err;
+        }
+
+        pfn = mfn_to_pfn(ctx, mfn);
+        vcpu.x64.ctrlreg[1] = 1 | ((uint64_t)pfn << PAGE_SHIFT);
+    }
+
+    if ( ctx->x86_pv.width == 8 )
+        rc = write_split_record(ctx, &rec, &vcpu, sizeof vcpu.x64);
+    else
+        rc = write_split_record(ctx, &rec, &vcpu, sizeof vcpu.x32);
+
+    if ( rc )
+        goto err;
+
+    DPRINTF("Writing vcpu%u basic context", id);
+    rc = 0;
+ err:
+
+    return rc;
+}
+
+static int write_one_vcpu_extended(struct context *ctx, uint32_t id)
+{
+    xc_interface *xch = ctx->xch;
+    int rc;
+    struct rec_x86_pv_vcpu vhdr = { .vcpu_id = id };
+    struct record rec =
+    {
+        .type = REC_TYPE_x86_pv_vcpu_extended,
+        .length = sizeof vhdr,
+        .data = &vhdr,
+    };
+    struct xen_domctl domctl =
+    {
+        .cmd = XEN_DOMCTL_get_ext_vcpucontext,
+        .domain = ctx->domid,
+        .u.ext_vcpucontext.vcpu = id,
+    };
+
+    if ( xc_domctl(xch, &domctl) < 0 )
+    {
+        PERROR("Unable to get vcpu%u extended context", id);
+        return -1;
+    }
+
+    rc = write_split_record(ctx, &rec, &domctl.u.ext_vcpucontext,
+                            domctl.u.ext_vcpucontext.size);
+    if ( rc )
+        return rc;
+
+    DPRINTF("Writing vcpu%u extended context", id);
+
+    return 0;
+}
+
+static int write_one_vcpu_xsave(struct context *ctx, uint32_t id)
+{
+    xc_interface *xch = ctx->xch;
+    int rc = -1;
+    DECLARE_HYPERCALL_BUFFER(void, buffer);
+    struct rec_x86_pv_vcpu_xsave vhdr = { .vcpu_id = id };
+    struct record rec =
+    {
+        .type = REC_TYPE_x86_pv_vcpu_xsave,
+        .length = sizeof vhdr,
+        .data = &vhdr,
+    };
+    struct xen_domctl domctl =
+    {
+        .cmd = XEN_DOMCTL_getvcpuextstate,
+        .domain = ctx->domid,
+        .u.vcpuextstate.vcpu = id,
+    };
+
+    if ( xc_domctl(xch, &domctl) < 0 )
+    {
+        PERROR("Unable to get vcpu%u's xsave context", id);
+        goto err;
+    }
+
+    if ( !domctl.u.vcpuextstate.xfeature_mask )
+    {
+        DPRINTF("vcpu%u has no xsave context - skipping", id);
+        goto out;
+    }
+
+    buffer = xc_hypercall_buffer_alloc(xch, buffer, domctl.u.vcpuextstate.size);
+    if ( !buffer )
+    {
+        ERROR("Unable to allocate %"PRIx64" bytes for vcpu%u's xsave context",
+              domctl.u.vcpuextstate.size, id);
+        goto err;
+    }
+
+    set_xen_guest_handle(domctl.u.vcpuextstate.buffer, buffer);
+    if ( xc_domctl(xch, &domctl) < 0 )
+    {
+        PERROR("Unable to get vcpu%u's xsave context", id);
+        goto err;
+    }
+
+    vhdr.xfeature_mask = domctl.u.vcpuextstate.xfeature_mask;
+
+    rc = write_split_record(ctx, &rec, buffer, domctl.u.vcpuextstate.size);
+    if ( rc )
+        goto err;
+
+    DPRINTF("Writing vcpu%u xsave context", id);
+
+ out:
+    rc = 0;
+
+ err:
+    xc_hypercall_buffer_free(xch, buffer);
+
+    return rc;
+}
+
+static int write_all_vcpu_information(struct context *ctx)
+{
+    xc_interface *xch = ctx->xch;
+    xc_vcpuinfo_t vinfo;
+    unsigned int i;
+    int rc;
+
+    for ( i = 0; i <= ctx->dominfo.max_vcpu_id; ++i )
+    {
+        rc = xc_vcpu_getinfo(xch, ctx->domid, i, &vinfo);
+        if ( rc )
+        {
+            PERROR("Failed to get vcpu%u information", i);
+            return rc;
+        }
+
+        if ( !vinfo.online )
+        {
+            DPRINTF("vcpu%u offline - skipping", i);
+            continue;
+        }
+
+        rc = write_one_vcpu_basic(ctx, i) ?:
+            write_one_vcpu_extended(ctx, i) ?:
+            write_one_vcpu_xsave(ctx, i);
+        if ( rc )
+            return rc;
+    };
+
+    return 0;
+}
+
+static int normalise_pagetable(struct context *ctx, const uint64_t *src,
+                               uint64_t *dst, unsigned long type)
+{
+    xc_interface *xch = ctx->xch;
+    uint64_t pte;
+    unsigned i, xen_first = -1, xen_last = -1; /* Indicies of Xen mappings */
+
+    type &= XEN_DOMCTL_PFINFO_LTABTYPE_MASK;
+
+    if ( ctx->x86_pv.levels == 4 )
+    {
+        /* 64bit guests only have Xen mappings in their L4 tables */
+        if ( type == XEN_DOMCTL_PFINFO_L4TAB )
+        {
+            xen_first = 256;
+            xen_last = 271;
+        }
+    }
+    else
+    {
+        switch ( type )
+        {
+        case XEN_DOMCTL_PFINFO_L4TAB:
+            ERROR("??? Found L4 table for 32bit guest");
+            errno = EINVAL;
+            return -1;
+
+        case XEN_DOMCTL_PFINFO_L3TAB:
+            /* 32bit guests can only use the first 4 entries of their L3 tables.
+             * All other are potentially used by Xen. */
+            xen_first = 4;
+            xen_last = 512;
+            break;
+
+        case XEN_DOMCTL_PFINFO_L2TAB:
+            /* It is hard to spot Xen mappings in a 32bit guest's L2.  Most
+             * are normal but only a few will have Xen mappings.
+             *
+             * 428 = (HYPERVISOR_VIRT_START_PAE >> L2_PAGETABLE_SHIFT_PAE) & 0x1ff
+             *
+             * ...which is conveniently unavailable to us in a 64bit build.
+             */
+            if ( pte_to_frame(ctx, src[428]) == ctx->x86_pv.compat_m2p_mfn0 )
+            {
+                xen_first = 428;
+                xen_last = 512;
+            }
+            break;
+        }
+    }
+
+    for ( i = 0; i < (PAGE_SIZE / sizeof(uint64_t)); ++i )
+    {
+        xen_pfn_t mfn, pfn;
+
+        pte = src[i];
+
+        /* Remove Xen mappings: Xen will reconstruct on the other side */
+        if ( i >= xen_first && i <= xen_last )
+            pte = 0;
+
+        if ( pte & _PAGE_PRESENT )
+        {
+            mfn = pte_to_frame(ctx, pte);
+
+            if ( pte & _PAGE_PSE )
+            {
+                ERROR("Cannot migrate superpage (L%lu[%u]: 0x%016"PRIx64")",
+                      type >> XEN_DOMCTL_PFINFO_LTAB_SHIFT, i, pte);
+                errno = E2BIG;
+                return -1;
+            }
+
+            if ( !mfn_in_pseudophysmap(ctx, mfn) )
+            {
+                /* This is expected during the live part of migration given
+                 * split pagetable updates, active grant mappings etc.  The
+                 * pagetable will need to be resent after pausing.  It is
+                 * however fatal if we have already paused the domain. */
+                if ( !ctx->dominfo.paused )
+                    errno = EAGAIN;
+                else
+                {
+                    ERROR("Bad MFN for L%lu[%u]",
+                          type >> XEN_DOMCTL_PFINFO_LTAB_SHIFT, i);
+                    pseudophysmap_walk(ctx, mfn);
+                    errno = ERANGE;
+                }
+                return -1;
+            }
+            else
+                pfn = mfn_to_pfn(ctx, mfn);
+
+            update_pte(ctx, &pte, pfn);
+        }
+
+        dst[i] = pte;
+    }
+
+    return 0;
+}
+
+static int write_batch(struct context *ctx)
+{
+    xc_interface *xch = ctx->xch;
+    xen_pfn_t *mfns = NULL, *types = NULL;
+    void *guest_mapping = NULL;
+    void **guest_data = NULL;
+    void **local_pages = NULL;
+    int *errors = NULL, rc = -1;
+    unsigned i, j, nr_pages = 0;
+    unsigned nr_pfns = ctx->nr_batch_pfns;
+
+    struct rec_page_data_header hdr;
+    uint32_t rec_type = REC_TYPE_page_data, rec_size, rec_count, rec_res = 0;
+    uint64_t *rec_pfns = NULL;
+    size_t s;
+
+    assert(nr_pfns != 0);
+
+    /* MFNs of the batch pfns */
+    mfns = malloc(nr_pfns * sizeof *mfns);
+    /* Types of the batch pfns */
+    types = malloc(nr_pfns * sizeof *types);
+    /* Errors from attempting to map the mfns */
+    errors = malloc(nr_pfns * sizeof *errors);
+    /* Pointers to page data to send.  Might be from mapped mfns or local allocations */
+    guest_data = calloc(nr_pfns, sizeof *guest_data);
+    /* Pointers to locally allocated pages.  Probably not all used, but need freeing */
+    local_pages = calloc(nr_pfns, sizeof *local_pages);
+
+    if ( !mfns || !types || !errors || !guest_data || !local_pages )
+    {
+        ERROR("Unable to allocate arrays for a batch of %u pages",
+              nr_pfns);
+        goto err;
+    }
+
+    for ( i = 0; i < nr_pfns; ++i )
+    {
+        types[i] = mfns[i] = pfn_to_mfn(ctx, ctx->batch_pfns[i]);
+
+        /* Likely a ballooned page */
+        if ( mfns[i] == INVALID_MFN )
+            set_bit(ctx->batch_pfns[i], ctx->deferred_pages);
+    }
+
+    rc = xc_get_pfn_type_batch(xch, ctx->domid, nr_pfns, types);
+    if ( rc )
+    {
+        PERROR("Failed to get types for pfn batch");
+        goto err;
+    }
+    rc = -1;
+
+    for ( i = 0; i < nr_pfns; ++i )
+    {
+        switch ( types[i] )
+        {
+        case XEN_DOMCTL_PFINFO_BROKEN:
+        case XEN_DOMCTL_PFINFO_XALLOC:
+        case XEN_DOMCTL_PFINFO_XTAB:
+            continue;
+        }
+
+        if ( !mfn_in_pseudophysmap(ctx, mfns[i]) )
+        {
+            ERROR("Bad mfn in for pfn %#lx, type %#lx",
+                  ctx->batch_pfns[i], types[i]);
+            pseudophysmap_walk(ctx, mfns[i]);
+            goto err;
+        }
+
+        mfns[nr_pages++] = mfns[i];
+    }
+
+    if ( nr_pages > 0 )
+    {
+        guest_mapping = xc_map_foreign_bulk(
+            xch, ctx->domid, PROT_READ, mfns, errors, nr_pages);
+        if ( !guest_mapping )
+        {
+            PERROR("Failed to map guest pages");
+            goto err;
+        }
+    }
+
+    for ( i = 0, j = 0; i < nr_pfns; ++i )
+    {
+        switch ( types[i] )
+        {
+        case XEN_DOMCTL_PFINFO_BROKEN:
+        case XEN_DOMCTL_PFINFO_XALLOC:
+        case XEN_DOMCTL_PFINFO_XTAB:
+            continue;
+        }
+
+        if ( errors[j] )
+        {
+            ERROR("Mapping of pfn %#lx (mfn %#lx) failed %d",
+                  ctx->batch_pfns[i], mfns[j], errors[j]);
+            goto err;
+        }
+
+        switch ( types[i] )
+        {
+        case XEN_DOMCTL_PFINFO_L1TAB:
+        case XEN_DOMCTL_PFINFO_L1TAB | XEN_DOMCTL_PFINFO_LPINTAB:
+
+        case XEN_DOMCTL_PFINFO_L2TAB:
+        case XEN_DOMCTL_PFINFO_L2TAB | XEN_DOMCTL_PFINFO_LPINTAB:
+
+        case XEN_DOMCTL_PFINFO_L3TAB:
+        case XEN_DOMCTL_PFINFO_L3TAB | XEN_DOMCTL_PFINFO_LPINTAB:
+
+        case XEN_DOMCTL_PFINFO_L4TAB:
+        case XEN_DOMCTL_PFINFO_L4TAB | XEN_DOMCTL_PFINFO_LPINTAB:
+
+            guest_data[i] = local_pages[i] = malloc(PAGE_SIZE);
+            if ( !local_pages[i] )
+            {
+                ERROR("Unable to allocate scratch page for pagetable normalisation");
+                goto err;
+            }
+
+            rc = normalise_pagetable(ctx, guest_mapping + (j * PAGE_SIZE),
+                                     local_pages[i], types[i]);
+            if ( rc )
+            {
+                if ( rc == -1 && errno == EAGAIN )
+                {
+                    set_bit(ctx->batch_pfns[i], ctx->deferred_pages);
+                    guest_data[i] = NULL;
+                    types[i] = XEN_DOMCTL_PFINFO_XTAB;
+                    --nr_pages;
+                }
+                else
+                    goto err;
+            }
+            rc = -1;
+            break;
+
+        case XEN_DOMCTL_PFINFO_NOTAB:
+            guest_data[i] = guest_mapping + (j * PAGE_SIZE);
+            break;
+        }
+
+        ++j;
+    }
+
+    hdr.count = nr_pfns;
+    s = nr_pfns * sizeof *rec_pfns;
+
+
+    rec_pfns = malloc(s);
+    if ( !rec_pfns )
+    {
+        ERROR("Unable to allocate memory for page data pfn list");
+        goto err;
+    }
+
+    for ( i = 0; i < nr_pfns; ++i )
+        rec_pfns[i] = ((uint64_t)(types[i]) << 32) | ctx->batch_pfns[i];
+
+    /*        header +          pfns data           +        page data */
+    rec_size = 4 + 4 + (s) + (nr_pages * PAGE_SIZE);
+    rec_count = nr_pfns;
+
+    if ( write_exact(ctx->fd, &rec_type, sizeof(uint32_t)) ||
+         write_exact(ctx->fd, &rec_size, sizeof(uint32_t)) ||
+         write_exact(ctx->fd, &rec_count, sizeof(uint32_t)) ||
+         write_exact(ctx->fd, &rec_res, sizeof(uint32_t)) )
+    {
+        PERROR("Failed to write page_type header to stream");
+        goto err;
+    }
+
+    if ( write_exact(ctx->fd, rec_pfns, s) )
+    {
+        PERROR("Failed to write page_type header to stream");
+        goto err;
+    }
+
+
+    for ( i = 0; i < nr_pfns; ++i )
+        if ( guest_data[i] )
+        {
+            if ( write_exact(ctx->fd, guest_data[i], PAGE_SIZE) )
+            {
+                PERROR("Failed to write page into stream");
+                goto err;
+            }
+
+            --nr_pages;
+        }
+
+    /* Sanity check */
+    assert(nr_pages == 0);
+    ctx->nr_batch_pfns = 0;
+    rc = 0;
+
+ err:
+    free(rec_pfns);
+    if ( guest_mapping )
+        munmap(guest_mapping, nr_pages * PAGE_SIZE);
+    for ( i = 0; local_pages && i < nr_pfns; ++i )
+        free(local_pages[i]);
+    free(local_pages);
+    free(guest_data);
+    free(errors);
+    free(types);
+    free(mfns);
+
+    return rc;
+}
+
+static int flush_batch(struct context *ctx)
+{
+    int rc = 0;
+
+    if ( ctx->nr_batch_pfns == 0 )
+        return rc;
+
+    rc = write_batch(ctx);
+
+    if ( !rc )
+    {
+        /* Valgrind sanity check */
+        free(ctx->batch_pfns);
+        ctx->batch_pfns = malloc(MAX_BATCH_SIZE * sizeof *ctx->batch_pfns);
+        rc = !ctx->batch_pfns;
+    }
+
+    return rc;
+}
+
+static int add_to_batch(struct context *ctx, xen_pfn_t pfn)
+{
+    int rc = 0;
+
+    if ( ctx->nr_batch_pfns == MAX_BATCH_SIZE )
+        rc = flush_batch(ctx);
+
+    if ( rc == 0 )
+        ctx->batch_pfns[ctx->nr_batch_pfns++] = pfn;
+
+    return rc;
+}
+
+static int write_x86_pv_info(struct context *ctx)
+{
+    struct rec_x86_pv_info info =
+        {
+            .guest_width = ctx->x86_pv.width,
+            .pt_levels = ctx->x86_pv.levels,
+        };
+    struct record rec =
+        {
+            .type = REC_TYPE_x86_pv_info,
+            .length = sizeof info,
+            .data = &info
+        };
+
+    return write_record(ctx, &rec);
+}
+
+static int write_x86_pv_p2m_frames(struct context *ctx)
+{
+    xc_interface *xch = ctx->xch;
+    int rc; unsigned i;
+    size_t datasz = ctx->x86_pv.p2m_frames * sizeof(uint64_t);
+    uint64_t *data = NULL;
+    struct rec_x86_pv_p2m_frames hdr =
+        {
+            .start_pfn = 0,
+            .end_pfn = ctx->x86_pv.max_pfn,
+        };
+    struct record rec =
+        {
+            .type = REC_TYPE_x86_pv_p2m_frames,
+            .length = sizeof hdr,
+            .data = &hdr,
+        };
+
+    /* No need to translate if sizeof(uint64_t) == sizeof(xen_pfn_t) */
+    if ( sizeof(uint64_t) != sizeof(*ctx->x86_pv.p2m_pfns) )
+    {
+        if ( !(data = malloc(datasz)) )
+        {
+            ERROR("Cannot allocate %zu bytes for X86_PV_P2M_FRAMES data", datasz);
+            return -1;
+        }
+
+        for ( i = 0; i < ctx->x86_pv.p2m_frames; ++i )
+            data[i] = ctx->x86_pv.p2m_pfns[i];
+    }
+    else
+        data = (uint64_t *)ctx->x86_pv.p2m_pfns;
+
+    rc = write_split_record(ctx, &rec, data, datasz);
+
+    if ( data != (uint64_t *)ctx->x86_pv.p2m_pfns )
+        free(data);
+
+    return rc;
+}
+
+static int write_shared_info(struct context *ctx)
+{
+    struct record rec =
+    {
+        .type = REC_TYPE_shared_info,
+        .length = PAGE_SIZE,
+        .data = ctx->x86_pv.shinfo,
+    };
+
+    return write_record(ctx, &rec);
+}
+
+static int write_tsc_info(struct context *ctx)
+{
+    xc_interface *xch = ctx->xch;
+    struct rec_tsc_info tsc = { 0 };
+    struct record rec =
+    {
+        .type = REC_TYPE_tsc_info,
+        .length = sizeof tsc,
+        .data = &tsc
+    };
+
+    if ( xc_domain_get_tsc_info(xch, ctx->domid, &tsc.mode,
+                                &tsc.nsec, &tsc.khz, &tsc.incarnation) < 0 )
+    {
+        PERROR("Unable to obtain TSC information");
+        return -1;
+    }
+
+    return write_record(ctx, &rec);
+}
+
+int save_x86_pv(struct context *ctx)
+{
+    xc_interface *xch = ctx->xch;
+    int rc;
+    struct record end = { REC_TYPE_end, 0, NULL };
+    xc_shadow_op_stats_t shadow_stats;
+    unsigned pages_written;
+
+    DECLARE_HYPERCALL_BUFFER(unsigned long, to_send);
+    unsigned x, max_iter = 5, dirty_threshold = 50;
+    xen_pfn_t p;
+
+    IPRINTF("In experimental %s", __func__);
+
+    /* Write Image and Domain headers to the stream */
+    rc = write_headers(ctx);
+    if ( rc )
+        goto err;
+
+    /* Query some properties, and stash in the save context */
+    rc = x86_pv_domain_info(ctx);
+    if ( rc )
+        goto err;
+
+    /* Write an X86_PV_INFO record into the stream */
+    rc = write_x86_pv_info(ctx);
+    if ( rc )
+        goto err;
+
+    /* Map various structures */
+    rc = x86_pv_map_m2p(ctx) ?: map_shinfo(ctx) ?: map_p2m(ctx);
+    if ( rc )
+        goto err;
+
+    /* Write a full X86_PV_P2M_FRAMES record into the stream */
+    rc = write_x86_pv_p2m_frames(ctx);
+    if ( rc )
+        goto err;
+    rc = -1;
+
+    ctx->batch_pfns = malloc(MAX_BATCH_SIZE * sizeof *ctx->batch_pfns);
+    if ( !ctx->batch_pfns )
+    {
+        ERROR("Unable to allocate memory for page batch list");
+        goto err;
+    }
+
+    to_send = xc_hypercall_buffer_alloc_pages(
+        xch, to_send, NRPAGES(bitmap_size(ctx->x86_pv.max_pfn + 1)));
+    ctx->deferred_pages = calloc(1, bitmap_size(ctx->x86_pv.max_pfn + 1));
+
+    if ( !to_send || !ctx->deferred_pages )
+    {
+        ERROR("Unable to allocate memory for to_{send,fix} bitmaps");
+        goto err;
+    }
+
+    memset(to_send, 0xff, bitmap_size(ctx->x86_pv.max_pfn + 1));
+
+    if ( (xc_shadow_control(xch, ctx->domid,
+                            XEN_DOMCTL_SHADOW_OP_ENABLE_LOGDIRTY,
+                            NULL, 0, NULL, 0, NULL) < 0) )
+    {
+        PERROR("Failed to enable logdirty");
+        goto err;
+    }
+
+    for ( x = 0, pages_written = 0; x < max_iter ; ++x )
+    {
+        DPRINTF("Iteration %u", x);
+
+        for ( p = 0 ; p <= ctx->x86_pv.max_pfn; ++p )
+        {
+            if ( test_bit(p, to_send) )
+            {
+                rc = add_to_batch(ctx, p);
+                if ( rc )
+                {
+                    ERROR("Fatal write error :s");
+                    goto err;
+                }
+
+                ++pages_written;
+            }
+        }
+
+        rc = flush_batch(ctx);
+        if ( rc )
+        {
+            ERROR("Fatal write error :s");
+            goto err;
+        }
+        rc = -1;
+
+        if ( xc_shadow_control(xch, ctx->domid,
+                               XEN_DOMCTL_SHADOW_OP_CLEAN,
+                               HYPERCALL_BUFFER(to_send),
+                               ctx->x86_pv.max_pfn + 1,
+                               NULL, 0, &shadow_stats) != ctx->x86_pv.max_pfn + 1)
+        {
+            PERROR("Failed to retrieve logdirty bitmap");
+            goto err;
+        }
+        else
+        {
+            DPRINTF("  Wrote %u pages; stats: faults %"PRIu32", dirty %"PRIu32,
+                    pages_written, shadow_stats.fault_count,
+                    shadow_stats.dirty_count);
+        }
+
+        if ( shadow_stats.dirty_count < dirty_threshold )
+            break;
+
+        pages_written = 0;
+    }
+
+    if ( !ctx->dominfo.paused )
+    {
+        rc = (ctx->save.callbacks->suspend(ctx->save.callbacks->data) != 1);
+        if ( rc )
+        {
+            ERROR("Failed to suspend domain");
+            goto err;
+        }
+    }
+
+    IPRINTF("Domain now paused");
+
+    if ( xc_shadow_control(xch, ctx->domid,
+                           XEN_DOMCTL_SHADOW_OP_CLEAN,
+                           HYPERCALL_BUFFER(to_send),
+                           ctx->x86_pv.max_pfn + 1,
+                           NULL, 0, &shadow_stats) != ctx->x86_pv.max_pfn + 1)
+    {
+        PERROR("Failed to retrieve logdirty bitmap");
+        goto err;
+    }
+
+    /*
+     * Domain must be paused from this point onwards.
+     */
+
+    for ( p = 0, pages_written = 0 ; p <= ctx->x86_pv.max_pfn; ++p )
+    {
+        if ( test_bit(p, to_send) || test_bit(p, ctx->deferred_pages) )
+        {
+            if ( add_to_batch(ctx, p) )
+            {
+                PERROR("Fatal error for pfn %lx", p);
+                goto err;
+            }
+            ++pages_written;
+        }
+    }
+    DPRINTF("  Wrote %u pages", pages_written);
+
+    rc = flush_batch(ctx);
+    if ( rc )
+    {
+        ERROR("Fatal write error :s");
+        goto err;
+    }
+
+    rc = write_tsc_info(ctx);
+    if ( rc )
+        goto err;
+
+    rc = write_shared_info(ctx);
+    if ( rc )
+        goto err;
+
+    /* Refresh domain information now it has paused */
+    if ( (xc_domain_getinfo(xch, ctx->domid, 1, &ctx->dominfo) != 1) ||
+         (ctx->dominfo.domid != ctx->domid) )
+    {
+        PERROR("Unable to refresh domain information");
+        rc = -1;
+        goto err;
+    }
+    else if ( (!ctx->dominfo.shutdown ||
+               ctx->dominfo.shutdown_reason != SHUTDOWN_suspend ) &&
+              !ctx->dominfo.paused )
+    {
+        ERROR("Domain has not been suspended");
+        rc = -1;
+        goto err;
+    }
+
+    /* Write all the vcpu information */
+    rc = write_all_vcpu_information(ctx);
+    if ( rc )
+        goto err;
+
+    /* Write an END record */
+    rc = write_record(ctx, &end);
+    if ( rc )
+        goto err;
+
+    /* all done */
+    assert(!rc);
+    goto cleanup;
+
+ err:
+    assert(rc);
+ cleanup:
+
+    xc_shadow_control(xch, ctx->domid, XEN_DOMCTL_SHADOW_OP_OFF,
+                      NULL, 0, NULL, 0, NULL);
+
+    free(ctx->deferred_pages);
+    xc_hypercall_buffer_free_pages(
+        xch, to_send, NRPAGES(bitmap_size(ctx->x86_pv.max_pfn + 1)));
+
+    free(ctx->x86_pv.p2m_pfns);
+
+    if ( ctx->x86_pv.p2m )
+        munmap(ctx->x86_pv.p2m, ctx->x86_pv.p2m_frames * PAGE_SIZE);
+
+    if ( ctx->x86_pv.shinfo )
+        munmap(ctx->x86_pv.shinfo, PAGE_SIZE);
+
+    if ( ctx->x86_pv.m2p )
+        munmap(ctx->x86_pv.m2p, ctx->x86_pv.nr_m2p_frames * PAGE_SIZE);
+
+    free(ctx->batch_pfns);
+
+    return rc;
+}
+
+/*
+ * Local variables:
+ * mode: C
+ * c-file-style: "BSD"
+ * c-basic-offset: 4
+ * tab-width: 4
+ * indent-tabs-mode: nil
+ * End:
+ */
